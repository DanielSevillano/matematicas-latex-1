\part{Inferencia no paramétrica}

\chapter{Inferencia no paramétrica}

\section{Introducción}

Hasta ahora los tests de hipótesis han sido utilizados para contrastar la veracidad de hipótesis acerca de los parámetros de la distribución de una población. Sin embargo,
en muchas ocasiones, es necesario emitir un juicio estadísstico sobre la distribución poblacional en su conjunto.

Los problemas de este tipo que se plantean de manera habitual son los siguientes:
\begin{itemize}
    \item Decidir, a la vista de una muestra aleatoria de una población, si puede admitirse que la distribución poblacional coincide con una cierta distribución dada o pertenece a un determinado tipo de distribuciones $\Rightarrow$ \textbf{Contrastes de bondad de ajuste}.
    \item Analizar si varias muestras aleatorias provienen de poblaciones con la misma distribución teórica, de forma que puedan ser utilizadas conjuntamente para inferencia posteriores acerca de ésta; o, por el contrario, son muestras de poblaciones con distinta distribución, que no pueden agruparse como información homogénea acerca de una única distribución $\Rightarrow$ \textbf{Contrastes de homogeneidad}.
    \item Estudiar, en el caso en que se observen dos o más características de los elementos de la población si las características observadas pueden considerarse independientes, y se puede proceder a su análisis por separado, o, por el contrario, existe relación estadística entre ellas $\Rightarrow$ \textbf{Contrastes de independencia}.
\end{itemize}

\section{Contrastes de bondad de ajuste}
Vamos a centrarnos en los siguientes contrastes:
\begin{itemize}
    \item Contraste $\chi^2$ de bondad de ajuste (primer caso).
    \item Contraste $\chi^2$ de bondad de ajuste (segundo caso).
\end{itemize}

\subsubsection{Contraste $\chi^2$ de bondad de ajuste (primer caso)}

Consideramos un muestra aleatoria $(X_1,\ldots, X_n)$ de una variable aleatoria $X$ con distribución desconocida. Queremos dar respuesta a:
\begin{itemize}
    \item ¿A la vista de la muestra, es razonable admitir que la distribución de $X$ viene dada por un determinado modelo de probabilidad $P$?
    \item ¿Se ajustan bien los datos a $P$?
\end{itemize}
Resolveremos el siguiente contraste de hipótesis:
\begin{align*}
    \begin{cases}
        H_0 : \text{El modelo de probabilidad de $X$ es $P$} \\
        H_1 : \text{El modelo de probabilidad de $X$ no es $P$}
    \end{cases}
\end{align*}
El modelo $P$ debe estar completamente especificado.

Para contrastar $H_0$ frente a $H_1$ hacemos una partición (arbitraria) del espacio muestral de la población (posibles valores de $X$) en $k$ clases $A_1, \ldots, A_k$. Después, para cada $A_i$, $i=1,\ldots,k$, consideramos las siguientes frecuencias (absolutas)
\begin{align*}
    O_i & = \text{frecuencia observada en $A_i$}                                         \\
        & = \text{número de elementos de la muestra que están en la clase $A_i$}         \\ \\
    e_i & = \text{frecuencia esperada de la clase $A_i$ si la hipótesis $H_0$ es cierta} \\
        & = n \cdot P(A_i)
\end{align*}
El estadístico que utilizaremos es:
\begin{align*}
    \sum_{i=1}^{k} \frac{(O_i - e_i)^2}{e_i},
\end{align*}
que tiene aproximadamente (cuando $n$ es grande) una distribución $\chi^2_{k-1}$ si $H_0$ es cierta.

Para que la aproximación sea razonablemente ''buena'', además de tener una muestra suficientemente grande, es necesario que el valor esperado de cada clase sea ''suficientemente grande''. Si la muestra procede de $P$, es de esperar que haya valores parecidos para $O_i$ y $e_i$, y por tanto, este estadístico debería tomar valores próximos a cero. En consecuencia, rechazaremos la hipótesis nula cuando los valores de este estadístico sean ''grandes'' y la aceptaremos cuando sean ''pequeños''.

Rechazaremos $H_0$ a nivel de significación $\alpha$ si
\begin{align*}
    \sum_{i=1}^{k} \frac{(O_i - e_i)^2}{e_i} > \chi^2_{k-1, 1 - \alpha}.
\end{align*}
En caso contrario, aceptaremos $H_0$ a nivel de significación $\alpha$.

\subsubsection{Contraste $\chi^2$ de bondad de ajuste (segundo caso)}

El contraste de bondad de ajuste se puede plantear también en una situación más general. Consideramos un muestra aleatoria $(X_1,\ldots, X_n)$ de una variable aleatoria $X$ con distribución desconocida. Queremos dar respuesta a:
\begin{itemize}
    \item ¿A la vista de la muestra, es razonable admitir que la distribución de $X$ viene dada por algún modelo de probabilidad de la familia $P_{\theta}$, donde $\theta = (\theta_1,\ldots,\theta_r)$?
    \item ¿Se ajustan bien los datos a un modelo de probabilidad de la familia $\{P_{\theta} : \theta \in \Theta\}$?
\end{itemize}
Resolveremos el siguiente contraste de hipótesis:
\begin{align*}
    \begin{cases}
        H_0 : \text{El modelo de probabilidad de $X$ es de la familia $\{P_{\theta} : \theta \in \Theta\}$} \\
        H_1 : \text{El modelo de probabilidad de $X$ no es de la familia $\{P_{\theta} : \theta \in \Theta\}$}
    \end{cases}
\end{align*}
Para contrastar $H_0$ frente a $H_1$ hacemos una partición (arbitraria) del espacio muestral de la población (posibles valores de $X$) en $k$ clases $A_1, \ldots, A_k$. Después, para cada $A_i$, $i=1,\ldots,k$, consideramos las siguientes frecuencias (absolutas)
\begin{align*}
    O_i & = \text{frecuencia observada en $A_i$}                                         \\
        & = \text{número de elementos de la muestra que están en la clase $A_i$}         \\ \\
    e_i & = \text{frecuencia esperada de la clase $A_i$ si la hipótesis $H_0$ es cierta} \\
        & = n \cdot P_{\theta}(A_i) \approx n \cdot P_{\widehat{\theta}}(A_i)
\end{align*}
donde $\widehat{\theta}$ es el estimador de máxima verosimilitud de $\theta$.

El estadístico que utilizaremos es:
\begin{align*}
    \sum_{i=1}^{k} \frac{(O_i - e_i)^2}{e_i},
\end{align*}
que tiene aproximadamente (cuando $n$ es grande) una distribución $\chi^2_{k-r-1}$ si $H_0$ es cierta.

Rechazaremos $H_0$ a nivel de significación $\alpha$ si
\begin{align*}
    \sum_{i=1}^{k} \frac{(O_i - e_i)^2}{e_i} > \chi^2_{k-r-1, 1 - \alpha}.
\end{align*}
En caso contrario, aceptaremos $H_0$ a nivel de significación $\alpha$.

\section{Contrastes de homogeneidad}

\subsubsection{Contraste $\chi^2$ de homogeneidad}

Supongamos que disponemos de $p$ muestras aleatorias independientes tomadas de $p$ poblaciones
\begin{align*}
    (X_{11}, X_{12} & \ldots, X_{1n_1}) \\
    (X_{21}, X_{22} & \ldots, X_{2n_2}) \\
                    & \quad \vdots      \\
    (X_{p1}, X_{p2} & \ldots, X_{pn_p})
\end{align*}
y sea $n = n_1 + n_2 + \ldots + n_p$. Queremos ver si, a la vista de las muestras obtenidas, es razonable admitir que todas las poblaciones tienen una distribución común, es decir, queremos ver si son poblaciones homogéneas. Por tanto, tenemos:
\begin{align*}
    \begin{cases}
        H_0 : \text{las $p$ poblaciones tienen una distribución común} \\
        H_1 : \text{las $p$ poblaciones no tienen una distribución común}
    \end{cases}
\end{align*}
Para contrastar $H_0$ frente a $H_1$ hacemos nuevamente una partición (arbitraria) del espacio muestral común a las $p$ poblaciones en $k$ clases $A_1,\ldots,A_k$. Después, definimos para la clase $A_i$, $i=1,\ldots,k$ y para la muestra de la población $j$-ésima ($j=1,\ldots,p$)
\begin{align*}
    O_{ij} & = \text{frecuencia observada en la clase $A_i$ con la $j$-ésima muestra}                                 \\ \\
    e_{ij} & = \text{frecuencia esperada en la clase $A_i$  con la $j$-ésima muestra si la hipótesis $H_0$ es cierta} \\
           & = n_j \cdot P_{\theta}(A_i)
\end{align*}
Entonces, tenemos para la muestra $j$-ésima
\begin{align*}
    \sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} \sim \chi^2_{k-1}, \quad (\text{aproximadamente}) \text{ si $H_0$ es cierta}
\end{align*}
Si sumamos los $p$ estadísticos obtenidos, uno para cada muestra, tenemos:
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} \sim \chi^2_{p(k-1)}, \quad (\text{aproximadamente}) \text{ si $H_0$ es cierta}
\end{align*}
Sin embargo, rodavía tenemos un problema por resolver: el valor de este se podría calcular si supiéramos cuál es la distribución $P$ común a las $p$ poblaciones. Normalmente, lo único que queremos contrastar es si tienen una distribución común, pero sin que sepamos, ni nos importe, cuál es esa distribución común puede ser cualquiera). Por tanto, tenemos que estimar $P(A_i)$, $i=1,\ldots,k$, a partir de las observaciones. Esta estimación se hace mediante:
\begin{align*}
    \widehat{P(A_i)} = \frac{\sum_{j=1}^{p} O_{ij}}{n}, \quad j = 1,\ldots,k.
\end{align*}
Las frecuencias esperadas serán, entonces
\begin{align*}
    e_{ij} = n_j \widehat{P(A_i)} = n_j  \frac{\sum_{j=1}^{p} O_{ij}}{n} = \frac{\left(\sum_{i=1}^{k} O_{ij}\right)\left(\sum_{j=1}^{p} O_{ij}\right)}{n}
\end{align*}
En definitiva, el estadístico utilizdo es, para los valores de las frecuencias esperadas dadas anteriormente
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} \sim \chi^2_{(p-1)(k-1)}, \quad (\text{aproximadamente}) \text{ si $H_0$ es cierta}
\end{align*}
Rechazaremos $H_0$ a nivel de significación $\alpha$ si
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} > \chi^2_{(p-1)(k-1), 1 - \alpha}.
\end{align*}
En caso contrario, aceptaremos $H_0$ a nivel de significación $\alpha$.

\section{Contrastes de independencia}

\subsubsection{Contraste $\chi^2$ de independencia}

Supongamos que queremos estudiar si dos características $X$ e $Y$ de una población están relacionadas o no. Para hacer este estudio, obtenemos una muestra aleatoria de $n$ pares de valores de estas características
\begin{align*}
    ((X_1,Y_1), \ldots, (X_n,Y_n)).
\end{align*}
Queremos ver si, a la vista de la muestra tiene sentido admitir que $X$ e $Y$ son independientes Por tanto, tenemos el siguiente contraste de hipótesis:
\begin{align*}
    \begin{cases}
        H_0 : \text{$X$ e $Y$ son independientes} \\
        H_1 : \text{$X$ e $Y$ no son independientes}
    \end{cases}
\end{align*}
Tomamos una partición (arbitraria) del espacio muestral (correspondientes a los posibles valores de $X$ e $Y$) en $k \cdot p$ clases $A_1 \times B_1, A_1 \times B_2, \ldots, A_k \times B_p$. Estas $k \cdot p$ clases corresponden a tomar clases $A_1, \ldots, A_k$ para la característica $X$, y las clases $B_1, \ldots, B_p$ para la característica $Y$.

Llamamos
\begin{align*}
    O_{ij} & = \text{frecuencia osbervada en la clase $A_i \times B_j$}                                \\ \\
    e_{ij} & = \text{frecuencia esperada en la clase $A_i \times B_j$ si la hipótesis $H_0$ es cierta} \\
           & = n \cdot P(A_i) \cdot P(B_j)
\end{align*}
Entonces tenemos que
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} \sim \chi^2_{(p-1)(k-1)}, \quad (\text{aproximadamente}) \text{ si $H_0$ es cierta}
\end{align*}
Pero otra vez tenemos el mismo problema de antes: los valores de $P(A_i)$ y $P(B_j)$ tienen que ser estimados a partir de la muestra; esto se hace de la forma:
\begin{align*}
    \widehat{P(A_i)} = \frac{\sum_{j=1}^{p} O_{ij}}{n}, \quad \widehat{P(B_j)} = \frac{\sum_{i=1}^{k} O_{ij}}{n}.
\end{align*}
Las frecuencias esperadas serán, entonces
\begin{align*}
    e_{ij} = n \cdot \widehat{P(A_i)} \cdot \widehat{P(B_j)} = n \cdot \frac{\sum_{j=1}^{p} O_{ij}}{n} \cdot  \frac{\sum_{i=1}^{k} O_{ij}}{n} = \frac{\left(\sum_{i=1}^{k} O_{ij}\right)\left(\sum_{j=1}^{p} O_{ij}\right)}{n}
\end{align*}
En definitiva, el estadístico utilizdo es, para los valores de las frecuencias esperadas dadas anteriormente
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} \sim \chi^2_{(p-1)(k-1)}, \quad (\text{aproximadamente}) \text{ si $H_0$ es cierta}
\end{align*}
Rechazaremos $H_0$ a nivel de significación $\alpha$ si
\begin{align*}
    \sum_{j=1}^{p}\sum_{i=1}^{k} \frac{(O_{ij} - e_{ij})^2}{e_{ij}} > \chi^2_{(p-1)(k-1), 1 - \alpha}.
\end{align*}
En caso contrario, aceptaremos $H_0$ a nivel de significación $\alpha$.
\begin{obs}
    Los contrastes $\chi^2$ tienen los siguientes inconvenientes:
    \begin{itemize}
        \item Son poco precisos para muestras pequeñas por ser tests asintóticos.
        \item Para variables continuas, se desprecia información al agrupar datos en clases..
    \end{itemize}
\end{obs}