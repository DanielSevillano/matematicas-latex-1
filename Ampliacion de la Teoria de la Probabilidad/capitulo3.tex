\chapter{Convergencia}
\noindent Consideremos un espacio de probabilidad $(\Omega, \mathcal{A}, P)$, $P : \mathcal{A} \longrightarrow [0,1]$. Sean $A_1,\ldots,A_n \in \mathbb{A}$ una sucesión de sucesos. Definimos el límite inferior y superior como sigue
\begin{align*}
    \liminf_{n} A_n = \bigcup_{n \ge 1} \bigcap_{m \ge n} A_m \in \mathcal{A}, \quad
    \limsup_{n} A_n = \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \in \mathcal{A}.
\end{align*}
Decimos que la sucesión $\{A_n\}$ converge si $\liminf_{n} A_n = \limsup_{n} A_n$. Algunos resultados que ya conocemos son los siguientes:
\begin{itemize}
    \item Toda sucesión monótona es convergente.
    \item Si $\{A_n\}$ es monótona creciente, entonces $\lim_{n} A_n = \cup_{i\ge1} A_i$.
    \item Si $\{A_n\}$ es monótona decreciente, entonces $\lim_{n} A_n = \cap_{i\ge1} A_i$.
\end{itemize}
\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión monótona creciente. Entonces
    \begin{align*}
        P\left( \lim_n A_n \right) = \lim_n P(A_n).
    \end{align*}
\end{teo}

\begin{proof}
    Como $\{A_n\}$ es creciente, entonces $\lim_{n} A_n = \cup_{i\ge1} A_i$, de donde:
    \begin{align*}
        P\left( \lim_n A_n \right) = P\left( \bigcup_{i\ge1} A_i \right).
    \end{align*}
    Como los $A_n$ no son disjuntos, definimos $F_n = A_n \backslash A_{n-1}$ para cada $n \ge 2$. Es claro que los $F_n$ son disjuntos y que
    \begin{align*}
        \bigcup_{i\ge1} A_i = A_1 \dot \bigcup F_2 \dot \bigcup \ldots \dot \bigcup F_n \dot \bigcup \ldots
    \end{align*}
    De aquí,
    \begin{align*}
        P \left( \bigcup_{i \ge 1} A_i\right) &= P(A_1) + P(F_2) + \ldots + P(F_n) + \ldots 
        = P(A_1) + \lim_{n \to \infty} \sum_{i=2}^{n} P(F_i) \\
        &= P(A_1) + \lim_{n \to \infty} (P(A_n) - P(A_1)) = \lim_{n \to \infty} A_n.
    \end{align*}
\end{proof}

\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión monótona decreciente. Entonces
    \begin{align*}
        P\left( \lim_n A_n \right) = \lim_n P(A_n).
    \end{align*}
\end{teo}

\begin{proof}
    Consideramos $\{A_n^c\}$, que es una sucesión monótoa decreciente. Por el Teorema anterior
    \begin{align*}
       P\left( \bigcup_{i \ge 1} A_i^c \right) = P\left( \lim_n A_n^c \right) = \lim_n P(A_n^c).
    \end{align*}
    Por las leyes de De Morgan:
    \begin{align*}
        P\left( \bigcup_{i \ge 1} A_i^c \right) = P\left( \left( \bigcap_{i \ge 1} A_i \right)^c \right) = 1 - P\left( \bigcap_{i \ge 1} A_i \right)
    \end{align*}
    Así,
    \begin{align*}
        \lim_{n \to \infty} P(A_n^c) = \lim_{n \to \infty} \left( 1 - P(A_n)\right) = 1 - P\left( \bigcap_{i \ge 1} A_i \right) \Longrightarrow \lim_{n \to \infty} P(A_n) = P \left( \bigcap_{n \ge 1} A_n \right)
    \end{align*}
\end{proof}

\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión. Entonces
    \begin{align*}
        P\left( \limsup_n A_n \right) = \lim_n P\left( \bigcup_{m \ge n} A_m \right).
    \end{align*}
\end{teo}

\begin{proof}
    Como $\limsup_n A_n = \cap_{n \ge 1} \cup_{m \ge n} A_m$, tenemos que $\left\{ \cup_{m \ge n} A_m \right\}$ es una sucesión decreciente y 
    \begin{align*}
        \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m = \lim_{n \to \infty} \left( \bigcup_{m \ge n} A_m \right),
    \end{align*}
    de donde concluimos que
    \begin{align*}
        P\left( \limsup_n A_n \right) = P\left( \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m  \right) = P\left( \lim_{n \to \infty} \left( \bigcup_{m \ge n} A_m \right) \right) = \lim_{n \to \infty} P\left( \bigcup_{m \ge n} A_m \right).
    \end{align*}
\end{proof}

\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión. Entonces
    \begin{align*}
        P\left( \liminf_n A_n \right) = \lim_n P\left( \bigcap_{m \ge n} A_m \right).
    \end{align*}
\end{teo}

\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión. Sea $\omega \in \Omega$, entonces $\omega \in \limsup_n A_n$ si y solo si existe una sucesión de índices $n_1 < n_2 < \ldots < n_k < \ldots $ tal que $\omega \in A_{n_k}$ para todo $k \in \mathbb{N}$.
\end{teo}

\begin{proof}
    $\boxed{\Longrightarrow}$ Supongamos que $\omega \in \limsup A_n$, es decir,
    \begin{align*}
        \omega \i \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m.
    \end{align*}
    En particular, $\omega \in \cup_{m \ge 1} A_m$, es decir, existe $\xi \in \mathbb{N}$, tal que $\omega \in A_{\xi}$. Tomamos $n_1 = \xi$. Por inducción sobre $k$, suponemos que $w \in A_{n_i}$, $i=1,...,k$. Actuando de igual forma, $\omega \in \cup_{m \ge n_{k+1}} A_m$, es decir, existe $\xi' \in \mathbb{N}$ tal que $\omega \in A_{\xi'}$ y tomamos $n_{k+1} = \xi'$.
    \\
    \newline
    $\boxed{\Longleftarrow}$ Por reducción al absurdo, supongamos que existe una sucesión de índices $n_1 < n_2 < \ldots < n_k < \ldots $ tal que $\omega \in A_{n_k}$ para todo $k \in \mathbb{N}$ y que $\omega \not \in \limsup A_n$. De esto últimos, tenemos que
    \begin{align*}
        \omega \not \in \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \Longrightarrow \omega \in \left( \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \right)^c \Longrightarrow \omega \in \bigcup_{n \ge 1} \bigcap_{m \ge n} A_m^c,
    \end{align*}
    es decir, existe $n_0 \in \mathbb{N}$ (fijo) tal que $\omega \in A_m^c$ para todo $m \ge n_0$, es decir, $\omega \in A_m$ para todo $m \ge n_0$, lo que nos dice que $\omega$ está en un número finito de $A_n$, los que es una contradicción.
\end{proof}

\begin{teo}
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión. Sea $\omega \in \Omega$, entonces $\omega \in \liminf_n A_n$ si y solo si existe $n_0 \in \mathbb{N}$ tal que $\omega \in A_m$ para todo $m \ge n_0$.
\end{teo}

\begin{teo}[Primer Lema de Borel-Cantelli]
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión tal que $\sum_{n \ge 1} P(A_n) < \infty$. Entonces
    \begin{align*}
        P\left( \limsup_n A_n \right) = 0.
    \end{align*}
\end{teo}

\begin{proof}
    \begin{align*}
        P\left( \limsup_n A_n \right) &= P \left( \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \right) \underset{(*)}{=} P \left( \lim_{n \to \infty} \bigcup_{m \ge n} A_m \right) \\
        &= \lim_{n \to \infty} P \left( \bigcup_{m \ge n} A_m \right) \leq \lim_{n \to \infty} \sum_{m \ge n} P(A_n) = 0.
    \end{align*}
    En $(*)$ estamos usando que la sucesión $\left\{ \cup_{m \ge n} A_m \right\}$ es decreciente.
\end{proof}

\begin{teo}[Segundo Lema de Borel-Cantelli]
     Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión tal que $\sum_{n \ge 1} P(A_n) = \infty$. Entonces
    \begin{align*}
        P\left( \limsup_n A_n \right) = 1.
    \end{align*}
\end{teo}

\begin{proof}
    \begin{align*}
        P\left( \limsup_n A_n \right) &= P \left( \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \right) = 1 - P\left[ \left( \bigcap_{n \ge 1} \bigcup_{m \ge n} A_m \right)^c \right] \\
        &= 1 - P\left(\bigcup_{n \ge 1} \bigcap_{m \ge n} A_m^c\right) = 1 - P\left(\lim_{n \to \infty} \bigcap_{m \ge n} A_m^c\right) \\
        &= 1 - \lim_{n \to \infty } P\left( \bigcap_{m \ge n} A_m^c\right) = 1 - \lim_{n \to \infty} \prod_{m \ge n} P(A_m^c) \\
        &= 1 - \lim_{n \to \infty} \prod_{m \ge} (1 - P(A_m)).
    \end{align*}
    Observamos que
    \begin{align*}
        \prod_{m \ge} (1 - P(A_m)) \underset{(*)}{=} \prod_{m \ge nn} e^{-P(A_m)}= e^{-\sum_{m \ge n}P(A_m)} = 0.
    \end{align*}
    En $(*)$ usamos que si $x \ge 0$, entonces $1 - x \leq e^{-x}$. Finalmente, gracias a lo anterior
        \begin{align*}
        P\left( \limsup_n A_n \right) = 1.
    \end{align*}
\end{proof}

\begin{cor}[Ley 0-1 de Borel-Cantelli]
    Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad y consideremos $\{A_n\} \subset \mathcal{A}$ una sucesión de sucesos independientes, entonces $P\left( \limsup_n A_n\right)$ o bien es 0, o bien es 1.
\end{cor}
\noindent Sean $X_n$ variables aleatorias en $(\Omega, \mathcal{A}, P)$. Definimos
\begin{align*}
    \Omega_1 = \{ \omega \in \Omega : \liminf_n X_n(\omega) = \limsup_n X_n(\omega) \},
\end{align*}
que es el conjunto de $\omega \in \Omega$ para los cuales existe $\lim_n X_n$.

\begin{teo}
    Sean $X_i$ variables aleatorias en $(\Omega, \mathcal{A}, P)$. Entonces $\inf_n X_n$, $\sup_n X_n$, \newline $\liminf_n X_n$ y $\limsup_n X_n$ son variables aleatorias.
\end{teo}

\begin{proof}
    Sea $Y = \inf_n X_n$ y fijemos $b \in \mathbb{R}$. Entonces
    \begin{align*}
        Y^{-1}((-\infty,b)) &= \left\{ \omega \in \Omega : \inf_n X_n(\omega) < b  \right\} = \bigcup_{n \ge 1} \left\{ \omega \in \Omega : X_n(\omega) < b  \right\} \\
        & \bigcup_{n \ge 1} X_n^{-1}((-\infty,b)) \in \mathcal{A},
    \end{align*}
    es decir, $Y$ es variable aleatoria. Observamos que $\sup_n X_n = - \inf_{n} (-X_n)$, \newline$\liminf_n X_n = \sup_n \inf_{m \ge n} A_m$ y $\limsup_n X_n = \inf_n \sup_{m \ge n} X_m$. Aplicar una función medible a una variable aleatoria, nos sigue dando una variable aleatoria, por tanto, todo lo anterior eran variables aleatorias.
\end{proof}

\begin{cor}
    $\Omega_1$ es medible.
\end{cor}

\begin{defi}
    Sea $\{X_n\}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$. Decimos que $X_n$ converge casi seguro si $P(\Omega_1) = 1$. En tal caso, $X:= \lim_n X_n$ y $X_n \xrightarrow[]{c.s} X$.
\end{defi}

\begin{teo}
    $X_n \xrightarrow[]{c.s} X$ si y solo si $P(\liminf_n y_{n.k}) = 1$ para todo $k$, siendo
    \begin{align*}
        y_{n,k} = \left\{ \omega \in \Omega : |X_n(\omega) - X(\omega)| < \frac{1}{k} \right\}.
    \end{align*}
\end{teo}

\begin{teo}
    $X_n \xrightarrow[]{c.s} X$ si y solo si $P(\limsup_n y_{n,k}^c) = 0$ para todo $k$.
\end{teo}

\begin{defi}
    Diremos que $X_n$ converge en probabilidad a $X$ si para todo $\varepsilon > 0$ 
    \begin{align*}
        P(y_{n,\varepsilon}) \xrightarrow[n\to \infty]{p} 1,
    \end{align*}
    siendo
    \begin{align*}
        y_{n,\varepsilon} = \left\{ \omega \in \Omega : |X_n(\omega) - X(\omega)| < \varepsilon \right\}.
    \end{align*}
\end{defi}

\begin{teo}
    El límite en probabilidad es único casi seguro (c.s).
\end{teo}

\begin{teo}
    Si $X_n \xrightarrow[]{c.s} X$, entonces $X_n \xrightarrow[]{p} X$.
\end{teo}

\begin{teo}
    Si $X_n \xrightarrow[]{p} X$, entonces existe alguna subsucesión $X_{n_k}$ de $X$ tal que $X_{n_k} \xrightarrow[]{c.s} X$.
\end{teo}

\begin{teo}
    $X_n \xrightarrow[]{p} X$ si y solo si toda subsucesión contiene una subsucesión convergente casi seguro.
\end{teo}

\begin{teo}
    La convergencia en probabilidad implica la convergencia en distribución.
\end{teo}

\begin{teo}
    Sean $X_n$, $X$ variables aleatorias en $(\Omega, \mathcal{A}, P)$ con $X \sim \delta(c)$, $c$ constante. Entonces $X_n \xrightarrow[]{p} X$ si y solo si $X_n \xrightarrow[]{d} X$.
\end{teo}

\noindent Consideremos el espacio 
\begin{align*}
    L^p(\Omega, \mathcal{A}, P) = \{ X \text{ v.a} : E[|X|^p] < \infty \}.
\end{align*}
Se puede probar que $(L^p, \|.\|_p)$ es un espacio métrico, siendo 
\begin{align*}
    \|X \|_p = \left( \int_{\Omega} |X|^p \ dP \right)^{1/p}
\end{align*}

\begin{defi}
    Diremos que $X_n$ converge a $X$ en $L^p$, $X_n \xrightarrow[]{L^p} X$, cuando $\|X_n - X\|_p \xrightarrow[n \to \infty]{} 0$.
\end{defi}
\noindent Diremos que
\begin{itemize}
    \item Converge en media si $p = 1$.
    \item Converge en media cuadrática si $p = 2$.
\end{itemize}
\noindent \underline{Desigualdad de Markov}: Sea $X$ variable no negativa y $a>0$, entonces
\begin{align*}
    P(X \ge a) \leq \frac{E[X]}{a}.
\end{align*}
\noindent Si además $X \in L^p$
\begin{align*}
    P(X \ge a) \leq \frac{E[X^p]}{a^p}.
\end{align*}
\begin{teo}
    $X_n, X \in L^p$. Si $X_n \xrightarrow[]{L^p} X$, entonces $X_n \xrightarrow[]{p} X$.
\end{teo}

\begin{teo}
    El límite en $L^p$ es único.
\end{teo}

\begin{teo}
    Sean $X_n, X \in L^p$ tales que $X_n \xrightarrow[]{p} X$. Si existe $Y \in L^p$ tal que $|X_n| \leq Y$ para todo $n$, entonces $X_n \xrightarrow[]{L^p} X$.
\end{teo}

\subsubsection{Ley débil de los grandes números (LDGN)}
\noindent Sean $X_1,X_2,\ldots$ variables aleatorias en $(\Omega, \mathcal{A}, P)$. Definimos
\begin{align*}
    S_n = X_1 + \ldots + X_n, \quad n \ge 1.
\end{align*}
La sucesión $\{X_n\}$ verifica la ley débil de los grandes números si existen sucesiones numéricas $\{a_n\}$ y $\{b_n\}$ con $b_n \uparrow \infty$ tales que
\begin{align*}
    \frac{S_n - a_n}{b_n} \xrightarrow[\quad]{p} 0.
\end{align*}

\begin{teo}[Bernoulli, 1713]
     Sean $X_1,X_2,\ldots$ variables aleatorias independientes en $(\Omega, \mathcal{A}, P)$ con $X_i \sim Ber(p)$, $0<p<1$. Entonces
     \begin{align*}
         \frac{S_n}{n} \xrightarrow[\quad]{p} p,
     \end{align*}
     es decir, verfica LDGN para $a_n = np$ y $b_n = n$.
\end{teo}

\begin{teo}[Chebyshev]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con media $\mu$ y varianza $\sigma^2$ constantes. Entonces
    \begin{align*}
        \frac{S_n}{n} \xrightarrow[\quad]{p} \mu
    \end{align*}
\end{teo}

\begin{teo}[Chebyshev]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con varianza acotada por una constante. Entonces
    \begin{align*}
        \frac{S_n - E[S_n]}{n} \xrightarrow[\quad]{p} 0
    \end{align*}
\end{teo}

\begin{teo}[Markov]
     Sea $\{X_n\}$ una sucesión de variables aleatorias independientes tal que $Var[S_n/n] \xrightarrow[n\to \infty]{} 0$. Entonces
    \begin{align*}
        \frac{S_n - E[S_n]}{n} \xrightarrow[\quad]{p} 0
    \end{align*}
\end{teo}

\begin{teo}[Khinchin]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes e identicamente distribuidas con media $\mu$ finita. Entonces
    \begin{align*}
        \frac{S_n}{n} \xrightarrow[\quad]{p} \mu
    \end{align*}
\end{teo}

\subsubsection{Ley fuerte de los grandes números (LFGN)}
\noindent Sean $X_1,X_2,\ldots$ variables aleatorias en $(\Omega, \mathcal{A}, P)$. La sucesión $\{X_n\}$ verifica la ley fuerte de los grandes números si existen sucesiones numéricas $\{a_n\}$ y $\{b_n\}$ con $b_n \uparrow \infty$ tales que
\begin{align*}
    \frac{S_n - a_n}{b_n} \xrightarrow[]{c.s} 0.
\end{align*}

\begin{lema}
    Sea $\{X_n\}$ sucesión de variables aleatorias. Entonces $\sum_{n=1}^{\infty} X_n$ converge casi seguro si y solo si
    \begin{align*}
        \lim_{n \to \infty} \lim_{m \to \infty} P\left( \max_{1 \leq j \leq m} |S_j - S_m| \ge \varepsilon \right) = 0
    \end{align*}
    para todo $\varepsilon > 0$.
\end{lema}

\begin{teo}[Criterio de convergencia de Kolmogórov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con $\sum_{n=1}^{\infty} Var[X_n] < \infty$. Entonces $\sum_{n=1}^{\infty}(X_n - E[X_n]) < \infty$ c.s.
\end{teo}

\begin{teo}[Recíproco de Kolmogórov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes. Si existe una constante $c>0$ tal que $|X_n| < c$ c.s para todo $n$. Entonces
    \begin{align*}
        \sum_{n=1}^{\infty}(X_n - E[X_n]) < \infty \text{ c.s} \Longleftrightarrow \sum_{n=1}^{\infty} Var[X_n] < \infty.
    \end{align*}
\end{teo}

\begin{cor}
    Sea $\{X_n\}$ una sucesión de variables aleatorias tales que $|X_n| < c$ c.s para algún $c>0$ constante. Si $\sum_{n=1}^{\infty} X_n < \infty$ c.s, entonces también convergen las series $\sum_{n=1}^{\infty}(X_n - E[X_n])$ c.s, $\sum_{n=1}^{\infty}E[X_n]$ y $\sum_{n=1}^{\infty} Var[X_n]$.
\end{cor}

\begin{teo}[Condición suficiente de Kolmogórov]
    Sea $\{X_n\}$ una suceción de variables aleatorias independienes con varianza finita. Si $\sum_{n=1}^{\infty} \frac{Var[X_n]}{n^2} < \infty$, entonces $\{X_n\}$ verifica la ley fuerte de los grandes números, es decir,
    \begin{align*}
        \frac{S_n}{n} \xrightarrow[]{c.s} 0
    \end{align*}
\end{teo}

\begin{lema}[Kronecker]
    Sea $\{X_n\}$ una sucesión de variables aleatorias y $\{a_n\}$ una sucesión de números reales tal que $a_n \uparrow \infty$. Si $\sum_{n=1}^{\infty} \frac{X_n}{a_n} < \infty$ c.s, entonces $\frac{1}{a_n} \sum_{k=1}^{n} X_k \xrightarrow[n\to \infty]{c.s} 0$. 
\end{lema}

\begin{defi}
    Sean $\{X_n\}$ una sucesión de variables aleatorias y $\{c_n\}$ una sucesión de números reales no negativos. Se define la sucesión de variables aleatorias truncadas como $\{Y_n\}$  donde
    \begin{align*}
        Y_n = X_n \mathbb{1}_{\{|X_n| < c_n\}}.
    \end{align*}
\end{defi}

\begin{defi}
    Dos sucesiones de variables aleatorias $\{X_n\}$ e $\{Y_n\}$ son equivalentes en convergencia cuando $\sum_{n=1}^{\infty} P(X_n \not = Y_n) = 0$.
\end{defi}

\begin{teo}
    Si $\{X_n\}$ e $\{Y_n\}$ son equivalentes en convergencia, entonces
    \begin{enumerate}
        \item $P\left( \limsup_n \{X_n \not = Y_n \}\right) = 0$.
        \item $\sum_{n=1}^{\infty} X_n < \infty$ c.s si y solo si $\sum_{n=1}^{\infty} Y_n < \infty$ c.s.
        \item $\frac{1}{n} \sum_{k=1}^{n} (X_k -Y_k) < \infty \xrightarrow[n \to \infty]{c.s} 0$. 
    \end{enumerate}
\end{teo}
\begin{teo}[3 series de Kolmogórov]
    Sean $\{X_n\}$ una sucesión de variables aleatorias independientes y $\{X_n^c\}$ una sucesión de variables aleatorias $X_n$ truncadas, para alguna constante $c>0$. Si existe $c>0$ tal que las series $\sum_{n=1}^{\infty} P(X_n \not = X_n^c)$, $\sum_{n=1}^{\infty} E[X_n^c]$ y $\sum_{n=1}^{\infty} Var[X_n^c]$ convergen, entonces $\sum_{n=1}^{\infty} X_n$ converge c.s.
    \\
    \newline
    Recíprocamete, si $\sum_{n=1}^{\infty} X_n$ converge c.s, entonces las tres series convergen para todo $c>0$.
\end{teo}

\begin{lema}
    Sea $X$ una variable aleatoria. Se tiene que $E[|X|] < \infty$ si y solo si \newline $\sum_{n=1}^{\infty} P(|X| \ge n) < \infty$.
\end{lema}

\begin{teo}[Ley fuerte de los grandes números]
    Sean $X_1,X_2,\ldots$ variables aleatorias independientes e igualmente distribuidas con $E[X_1] = \mu< \infty$, entonces
    \begin{align*}
        \frac{S_n}{n} \xrightarrow[]{c.s} \mu.
    \end{align*}
    Recíprocamente, si $\frac{S_n}{n} \xrightarrow[]{c.s} c$ (constante), entonces $E[X_1] = c$.
\end{teo}

\begin{teo}[Teorema central del límite]
    Sean $X_1, X_2, \ldots$ variables aleatorias, independientes e idénticamete distribuidas con $E[X_1] = \mu < \infty$ y $Var[X_1] = \sigma^2 < \infty$. Entonces
    \begin{align*}
        \frac{\sqrt{n}}{\sigma} \left( \frac{S_n}{n} - \mu \right) \xrightarrow[\quad]{d} Z \sim N(0,1).
    \end{align*}
\end{teo}

\begin{teo}[TCL de Lindeberg-Feller]
    Sean $X_1, X_2, \ldots$ variables aleatorias, independientes con $E[X_n] = \mu_n < \infty$ y $Var[X_n] = \sigma^2_n < \infty$. Definimos $s_n^2 := Var[S_n] = \sum_{j=1}^{n} \sigma^2_j$. Entonces
    \begin{enumerate}
        \item $\frac{1}{s_n} \sum_{j=1}^{n} (X_j - \mu_j) \xrightarrow[\quad]{d} Z \sim N(0,1)$,
        \item $\max_{1 \leq j \leq n} \frac{\sigma^2_j}{s_n^2} \xrightarrow[n \to \infty]{} 0$
    \end{enumerate}
    es equivalente a
    \begin{align*}
        L_n(\varepsilon) = \frac{1}{s_n^2} \sum_{j=1}^{n} \int_{|x - \mu_j| > \varepsilon  s_n}(x-\mu_j)^2 dF_j(x) \xrightarrow[n \to \infty]{} 0.
    \end{align*}
\end{teo}