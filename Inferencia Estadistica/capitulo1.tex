\chapter{Introducción a la Inferencia}

\section{Tipos de inferencia}

La \textbf{Estadística} es la rama de las Matemáticas que se ocupa de recoger datos, analizarlos y organizarlos, y de realizar predicciones que sobre esos datos puedan deducirse. Tiene dos vertientes básicas:
\begin{enumerate}
    \item[a)] \textbf{Estadística Descriptiva:} Se ocupa de, a partir de ciertos datos, analizarlos y organizarlos. Es aquí donde tiene sentido calcular la media, mediana, moda, desviación media, desviación típica, etc.
    \item[b)] \textbf{Estadística Inferencial:} Se ocupa de predecir, sacar conclusiones, para una población tomando como base una muestra de dicha población. Como todas las predicciones, siempre han de hacerse bajo un cierto grado de fiabilidad o confianza.
\end{enumerate}
En un sentido amplio, se entiende por \textbf{Inferencia} a la parte de la Estadística que estudia grandes colectivos (población) a partir de una pequeña parte de ésta (muestra).

Un factor clave de diferenciación entre los diversos problemas de Inferencia Estadística. que pueden plantearse, es el tipo de conclusiones que se quieren establecer sobre la situación aleatoria en cuestión. Distinguimos entonces:

\begin{itemize}
    \item \textbf{Estimación puntual:} Situaciones en las que el objetivo es utilizar la información suministrada por las observaciones para obtener un pronóstico numérico único acerca de un determinado parámetro de la distribución de la característica o población en estudio.
    \item \textbf{Estimación por intervalos:} Situaciones en las que el objetivo es proporcionar un margen de variación para un determinado parámetro de la distribución desconocida; es decir: precisar un intervalo numérico en el que pueda afirmarse. razonablemente, que varía. el parámetro en cuestión.
    \item \textbf{Contrastes de Hipótesis:} Tratar de corroborar o invalidar una determinada afirmación acerca de la distribución de probabilidad del fenómeno estudiado.
\end{itemize}
La población en estudio viene representada por una variable aleatoria, $X$, con una determinada distribución de probabilidad $X \sim F_X$. Dependiendo del grado de conocimiento de esta distribución, se distinguen dos métodos para realizar procesos de inferencia.

\begin{enumerate}
    \item[a)] \textbf{Inferencia Paramétrica:} Es aquella en la que se admite que la distribución de la población pertenece a una cierta familia paramétrica de distribuciones, siendo necesario únicamente precisar el valor de los parámetros para determinar la distribución poblacional.
          \begin{enumerate}
              \item[1)] \textbf{Enfoque clásico:} Los parámetros de la distribución de la población se consideran constantes.
              \item[2)] \textbf{Enfoque bayesiano:} Considera los parámetros de la distribución como variables aleatorias permitiendo introducir información sobre ellos a través de la distribución a priori.
          \end{enumerate}
    \item[b)] \textbf{Inferencia no Paramétrica:} No supone ninguna distribución de probabilidad de la población, exigiendo solo hipótesis muy generales como puede ser la de simetría de la distribucion. Los procedimientos no paramétricos se pueden clasificiar en:
          \begin{enumerate}
              \item[1)] \textbf{Procedimientos de localización}, que estudian los parámetros de localización de la distribución.
              \item[2)] \textbf{Procedimientos de estructura}, que estudian las condiciones que se dan en la distribución de la variable.
              \item[3)] \textbf{Procedimientos sobre las condiciones de la muestra}, que comprueban si se verifican las hipótesis exigibles a los valores muestrables como pueden ser la independencia, ausencia de valores atípicos, etc.
          \end{enumerate}
\end{enumerate}

\section{Elementos de la Inferencia Estadística}

Todo problema de inferencia estadística está motivado por un cierto grado de desconocimiento de la ley de probabilidad que rige cierto fenómeno aleatorio. El caso más simple es cuando el interés se centra en una determinada variable aleatoria $X$ cuya distribución $F$ es más o menos desconocida. La distribución $F$ involucrada en el problema de inferencia estadística se denomina \textit{distribución teórica} o \textit{distribución de la población}.

Los elementos de la Inferencia Estadísitca son:
\begin{itemize}
    \item \textbf{Población:} Conjunto de elementos sobre los que se observa una característica común. Esta característica viene representada por una variable aleatoria cuya distribución $F$ es desconocida o no se conoce su totalidad.
    \item \textbf{Muestra:} Conjunto de unidades de una población. Cuanto más significativa sea, mejor será la muestra.
    \item \textbf{Parámetros poblacionales:} Son los índices centrales y de dispersión que definen a una población.
\end{itemize}
En los modelos de Inferencia Estadística el mayor o menor grado de desconocimiento acerca de la distribución teórica $F$ se refleja mediante una familia $\mathcal{F}$ de distribuciones. La situación más simple es aquella en la que la familia $\mathcal{F}$ está compuesta por distribuciones que tienen una forma funcional fija y conocida y que dependen de un parámetro $\theta$, de una o más dimensiones, que varía en un subconjunto $\Theta \subset \mathbb{R}^k$, llamado \textbf{espacio paramétrico}
\begin{align*}
    \mathcal{F} = \{ F_{\theta} : \theta \in \Theta \in \mathbb{R}^k \}.
\end{align*}

\section{Muestreo aleatorio}
Para que la muestra sea representativa debe reflejar las similitudes y diferencia encontradas en la población. Los errores más comunes que se pueden cometer son:
\begin{enumerate}
    \item[1)] \textbf{Error de muestreo:} Hacer conclusiones muy generales a partir de la observación de sólo una parte de la población.
    \item[2)] \textbf{Error de inferencia:} Hacer conclusiones hacia una población de mayor tamaño que de la que se tomó la muestra.
\end{enumerate}
Los métodos para muestreo se clasifican en:
\begin{enumerate}
    \item[(i)] Muestreos probabilísticos.
    \item[(ii)] Muestreos no probabilísticos.
\end{enumerate}

\subsection{Muestreo probabilístico}
Los muestreos probabilísticos se basan en el principio de equiprobabilidad: todos los individuos tienen la misma probabilidad de ser elegidos para formar parte de una muestra.
\begin{itemize}
    \item \textbf{Muestreo aleatoria simple con reemplazamiento:} Todas las unidades muestrales tienen la misma probabilidad de ser elegidas, pudiendo medirse varias veces el mismo individuo.
    \item \textbf{Muestreo aleatoria simple sin reemplazamiento}: Todas las unidades muestrales tienen la misma probabilidad de ser elegidas, pero los individuos no tienen posibilidad de aparecer en la muestra más de una vez.
    \item \textbf{Muestreo estratificado:} La población está dividida en subpoblaciones o estratos que contienen elementos parecidos entre sí. La composición de la muestra se distribuye entre los distintos estratos mediante un procedimiento que recibe el nombre de \textit{afijación}.
          \begin{enumerate}
              \item[a)] \textbf{Afijación uniforme:} En la muestra habrá el mismo número de representantes por cada estrato. Si hay $k$ estratos y el tamaño de la muestra es $n$, se extraerán aproximadamente $n/k$ elementos de cada estrato.
              \item[b)] \textbf{Afijación proporcional:} En la muestra habrá un número de representantes de cada estrato proporcional a su tamaño. Si el estrato $i$ tiene $N_i$ elementos y la población total está formada por $N$ elementos, al estrato $i$ le corresponden $(N_i/N) \cdot n$ elementos muestrales, donde $n$ es el tamaño de muestra que se quiere tomar.
              \item[c)] La asignación de unidades muestrales se hace teniendo en cuenta tanto el \textbf{tamaño de la muestra} como su \textbf{variabilidad}. De esta forma en un estrato más heterogéneo se necesitan más unidades muestrales mientras que en uno más homogéneo se necesitan menos unidades muestrales.

                    Si $\sigma_i$ representa a la desviación típica del $i$-ésimo estrato, la asignación de las unidades muestrales para dicho estrato será
                    \begin{align*}
                        n_i = n \cdot \frac{N_i \sigma_i}{\sum_{j=1}^{k}{\sigma_j N_j}}.
                    \end{align*}
          \end{enumerate}
    \item \textbf{Muestreo por conglomerados:} Dada una población, se establecen grupos de elementos físicamente próximos entre ellos frecuentemente constituidos por una partición geográfica de la población. Los conglomerados deben ser tan heterogéneos como la población a estudiar para que la muestra la represente bien.

          Obsérvese que en el muestreo estratificado un estrato es homogéneo (sus elementos tienen la mismas características), mientras que un conglomerado es heterogéneo porque debe representar a la población.
    \item \textbf{Muestreo sistemático:} Para obtener una muestra de tamaño $n$ se elige un individuo al azar y a partir de él, a intervalos constantes, se eligen los demás hasta completar la muestra.
\end{itemize}

\subsection{Muestreo no probabilístico}
Cuando el muestreo probabilístico resulta costoso o no se tiene asegurado que cualquier elemento de la población pueda pertenecer a una muestra, se consideran muestreos no probabilísticos. Estos métodos no sirven para realizar generalizaciones pues no se tiene la certeza de que la muestra sea representativa.

En este tipo de muestreo se da preferencia en la muestra a aquellos individuos que al cumplir con cierta cualidad o característica benefician a la investigación.
\begin{itemize}
    \item \textbf{Muestreo por cuotas:} Se supone que existe un buen conocimiento de los estratos de la población y/o los individuos más representativos o adecuados para la investigación. Tiene semenjanzas con el muestreo estratificado pero no tiene el carácter de aleatoriedad.
    \item \textbf{Muestreo intencional o de conveniencia:} Este muestreo se caracteriza por intentar obtener muestras representativas mediante la inclusión en la muestra de grupos típicos. Es utilizado en sondeos preelectorales de zonas que en anteriores votaciones han marcado tendencias de voto.
    \item \textbf{Bola de nieve: } Se localiza a algunos individuos que a su vez conducen a otrso, y estos a otros, y así sucesivamente hasta conseguir una muestra suficiente. Se usa cuando se hacen estudios en poblaciones marginales, delincuentes, sectas, determinados tipos de enfermos, etc.
\end{itemize}

\section{Muestra aleatoria simple}

\begin{defi}
    Una \textbf{muestra aleatoria simple de tamaño n} de una variable aleatoria $X$ con distribución teórica $F_X$, es un conjunto de $n$ variables aleatorias independientes $(X_1, ..., X_n)$ e igualmente distribuidas con distribución común $F_X$.
\end{defi}
Cada $X_i$ es una variable aleatoria que representa la característica bajo estudio del elemento $i$-ésimo de la muestra. Cuando las mediciones se hayan llevado a cabo, es decir, una vez realizado el muestreo, los resultados obtenidos, $(x_1, ..., x_n)$, se denomiarán \textbf{realización de la muestra}.

Como las variables aleatorias han de ser independientes, la función de distribución conjunta de la muestra aleatoria simple será
\begin{align*}
    F(x_1,...,x_n) = F_X(x_1) \dotsb F_X(x_n).
\end{align*}
No habrá ambig\"uedad en representar mediante el mismo símbolo, $F$, la función de distribución conjunta y las marginales puesto que el número de variables las distingue.

\section{Concepto de estadístico y su distribución en el muestreo}

\begin{defi}
    Llamamos \textbf{estadístico} a cualquier función medible de las variables aleatorias observables de la muestra aleatoria simple que no depende de ningún parámetro desconocido en el estudio.
\end{defi}

En general un estadístico es una función medible $T: \mathbb{R}^n \longrightarrow \mathbb{R}^k$ aplicada a la muestra aleatoria simple, $T(X_1, ..., X_n)$.

Un estadístico $T$ es una nueva varible aleatoria (o vector aleatorio) que tendrá una distribución asociada llamada \textit{distribución muestral} con una función de distribución llamada \textit{función de distribución empírica}.

\subsection{Estadísticos más utilizados}
Los estadísticos más utilizados son:
\begin{itemize}
    \item \textbf{Media muestral}
          \begin{align*}
              T(X_1,X_2, ..., X_n) = \overline{X} = \frac{\sum_{i=1}^{n}{X_i}}{n}
          \end{align*}
    \item \textbf{Varianza muestral}
          \begin{align*}
              T(X_1,X_2, ..., X_n) = \var_n(\textbf{X}) = \frac{\sum_{i=1}^{n}{(X_i - \overline{X})^2}}{n}
          \end{align*}
    \item \textbf{Cuasivarianza muestral}
          \begin{align*}
              T(X_1,X_2, ..., X_n) = \text{S}^2 = \frac{\sum_{i=1}^{n}{(X_i - \overline{X})^2}}{n-1}
          \end{align*}
    \item \textbf{Máximo y Mínimo}
          \begin{align*}
               & T(X_1,X_2, ..., X_N) = X_{(n)}=  \max(X_1, X_2, ...,X_n) \\
               & T(X_1,X_2, ..., X_N) = X_{(1)} =\min(X_1, X_2, ...,X_n)
          \end{align*}
\end{itemize}

\begin{obs}
    \begin{align*}
        \text{S}^2 = \frac{n}{n-1}\var_n(\textbf{X})
    \end{align*}
\end{obs}

\subsection{Función de distribución muestral}

Sea $X \sim F_{\theta}$, donde $\{F_{\theta} : \theta \in \Theta \}$.  Identifiquemos $F_{\theta} \equiv F$. ¿Cómo podemos estimar $F$?

Tomamos $(X_1, X_2,..., X_n)$ una mumestra aleatoria simple ($X_i \sim F$, $i = 1,...,n$ e independientes). Definimos
\begin{align*}
    F_{(X_1, X_2,..., X_n)}^*(x) = \frac{\text{número de variables tales que } X_i \leq x}{n}, \ x \in \mathbb{R},
\end{align*}
que es una nueva variable aleatoria. Observamos que podemos escribir dicha variable aleatoria de la siguiente forma
\begin{align*}
    F_{(X_1, X_2,..., X_n)}^*(x) = \frac{1}{n}\sum_{i=1}^{n}{I_{(-\infty,x]}(X_i)}.
\end{align*}
Si llamamos
\begin{align*}
    Y_i = I_{(-\infty,x]}(X_i) = \left\{ \begin{array}{lcc}
                                             1 & si & X_i \leq x \\
                                             0 & si & X_i > x    \\
                                         \end{array}
    \right.
\end{align*}
es una variable aleatoria e $Y_i \sim Ber(p)$. Encontremos dicho $p$. Observamos que
\begin{align*}
    P(Y_i = 1) = P(X_i \leq x) = F_{X_i}(x) = F(x).
\end{align*}
Entonces
\begin{align*}
    F_{(X_1, X_2,..., X_n)}^*(x) = \frac{1}{n}\sum_{i=1}^{n}{Y_i},
\end{align*}
como $Y_i$ son variables aleatorias independientes, entonces
\begin{align*}
    nF_{(X_1, X_2,..., X_n)}^*(x) = \sum_{i=1}^{n}{Y_i} \sim Bi(n, F(x)).
\end{align*}
De aquí deducimos que
\begin{align*}
     & E(F_{(X_1, X_2,..., X_n)}^*(x)) = \frac{1}{n}nF(x) = F(x)                                  \\
     & V(F_{(X_1, X_2,..., X_n)}^*(x)) = \frac{1}{n^2}nF(x)(1 - F(x)) = \frac{1}{n}F(x)(1 - F(X)) \\
\end{align*}
Además, por el Teorema Central del Límite
\begin{align*}
    F_{(X_1, X_2,..., X_n)}^*(x) \sim N\left( F(x), \sqrt{\frac{F(x)(1 - F(x))}{n}}\right)
\end{align*}

\subsection{Momentos centrales y no centrales}

Sea $X \sim F_X$ y sea $(X_1, X_2,..., X_n)$ una mumestra aleatoria simple ($X_i \sim F$, $i = 1,...,n$ e independientes). Entonces
\begin{itemize}
    \item \textbf{Momento ordinario de orden k respecto del origen}
          \begin{align*}
              m_k = \frac{\sum_{i=1}^{n}{X_i^k}}{n}
          \end{align*}
    \item \textbf{Momento central de orden k}
          \begin{align*}
              M_k = \frac{\sum_{i=1}^{n}{(X_i - \overline{X})^k}}{n}
          \end{align*}
\end{itemize}

